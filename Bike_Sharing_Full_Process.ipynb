{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e30b1c5",
   "metadata": {},
   "source": [
    "# Bike Sharing Rental Demand Prediction\n",
    "This notebook contains the complete end-to-end process from Data Cleaning to Model Enhancement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c196f",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis & Cleaning\n",
    "In this step, we handle missing values, correct data types, and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c18c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg') # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create images directory if it doesn't exist\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('Dataset.csv', encoding='latin1')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 1. Inspect Data Types and Clean\n",
    "print(\"\\n--- Initial Info ---\")\n",
    "print(df.info())\n",
    "\n",
    "# Columns that should be numeric but are object\n",
    "numeric_candidates = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered']\n",
    "\n",
    "print(\"\\n--- Checking for non-numeric values ---\")\n",
    "for col in numeric_candidates:\n",
    "    # Force convert to numeric, trace errors\n",
    "    temp_series = pd.to_numeric(df[col], errors='coerce')\n",
    "    n_errors = temp_series.isna().sum()\n",
    "    if n_errors > 0:\n",
    "        print(f\"Column '{col}' has {n_errors} non-numeric entries (will be converted to NaN).\")\n",
    "        # specific examples\n",
    "        invalid_mask = pd.to_numeric(df[col], errors='coerce').isna()\n",
    "        print(f\"Examples: {df.loc[invalid_mask, col].unique()[:5]}\")\n",
    "    \n",
    "    # Apply conversion\n",
    "    df[col] = temp_series\n",
    "\n",
    "# Convert dteday to datetime\n",
    "df['dteday'] = pd.to_datetime(df['dteday'], errors='coerce')\n",
    "\n",
    "# Check for missing values after conversion\n",
    "print(\"\\n--- Missing Values After Cleaning ---\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# Fill missing values if any (for now just print, user decided strategy)\n",
    "# Strategy: user said \"Handle missing values: Detect... and apply appropriate imputation\"\n",
    "# If simple errors, maybe drop? Or impute?\n",
    "# Let's see how many first.\n",
    "\n",
    "# 2. Outlier Detection (Boxplots)\n",
    "print(\"\\n--- Generating Outlier Boxplots ---\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Use cleaned numeric cols plus 'cnt'\n",
    "plot_cols = numeric_candidates + ['cnt']\n",
    "# ensure they are in df\n",
    "plot_cols = [c for c in plot_cols if c in df.columns]\n",
    "\n",
    "for i, col in enumerate(plot_cols, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(y=df[col].dropna())\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/outliers_boxplot_cleaned.png')\n",
    "print(\"Saved images/outliers_boxplot_cleaned.png\")\n",
    "\n",
    "# 3. Correlation Matrix\n",
    "print(\"\\n--- Generating Correlation Matrix ---\")\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Select only numeric columns\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "if not numeric_df.empty:\n",
    "    sns.heatmap(numeric_df.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.savefig('images/correlation_matrix.png')\n",
    "    print(\"Saved images/correlation_matrix.png\")\n",
    "else:\n",
    "    print(\"No numeric columns for correlation matrix.\")\n",
    "\n",
    "# Save cleaned data for next steps\n",
    "df.to_csv('cleaned_dataset.csv', index=False)\n",
    "print(\"\\nSaved cleaned dataset to 'cleaned_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fddc3",
   "metadata": {},
   "source": [
    "## 2. Data Visualization\n",
    "Visualizing patterns in demand across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# Load the cleaned and processed data\n",
    "df_clean = pd.read_csv('cleaned_bike_data.csv')\n",
    "df_proc = pd.read_csv('processed_bike_data.csv')\n",
    "model = joblib.load('models/best_random_forest.joblib')\n",
    "\n",
    "# --- 1. Rental Distribution ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_clean['cnt'], kde=True, color='purple')\n",
    "plt.title('Distribution of Total Bike Rentals')\n",
    "plt.xlabel('Rental Count')\n",
    "plt.savefig('images/dist_cnt.png')\n",
    "\n",
    "# --- 2. Hourly Demand ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='hr', y='cnt', data=df_clean, ci=None, marker='o')\n",
    "plt.title('Average Hourly Demand Pattern')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Count')\n",
    "plt.savefig('images/hourly_trend.png')\n",
    "\n",
    "# --- 3. Seasonal Demand ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='season', y='cnt', data=df_clean, palette='viridis')\n",
    "plt.title('Rental Counts by Season')\n",
    "plt.savefig('images/seasonal_demand.png')\n",
    "\n",
    "# --- 4. Weather Situation Impact ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='weathersit', y='cnt', data=df_clean)\n",
    "plt.title('Impact of Weather Status on Rentals')\n",
    "plt.savefig('images/weather_impact.png')\n",
    "\n",
    "# --- 5. Temperature Influence ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='temp', y='cnt', data=df_clean, alpha=0.3, color='orange')\n",
    "plt.title('Normalized Temperature vs Rental Count')\n",
    "plt.savefig('images/temp_vs_count.png')\n",
    "\n",
    "# --- 6. Working Day vs Holiday ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='workingday', y='cnt', hue='holiday', data=df_clean)\n",
    "plt.title('Rentals: Working Day vs Holiday comparison')\n",
    "plt.savefig('images/workingday_holiday.png')\n",
    "\n",
    "# --- 7. Residual Plot (Actual vs Predicted) ---\n",
    "target = 'cnt'\n",
    "X = df_proc.drop(columns=['casual', 'registered', 'cnt'], errors='ignore')\n",
    "y = df_proc[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color='teal')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title('Actual vs Predicted Count')\n",
    "plt.xlabel('Actual Count')\n",
    "plt.ylabel('Predicted Count')\n",
    "plt.savefig('images/actual_vs_predicted.png')\n",
    "\n",
    "# --- 8. Feature Importance (Rich version) ---\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_df.head(20), palette='magma')\n",
    "plt.title('Top 20 Drivers of Bike Demand')\n",
    "plt.savefig('images/top_drivers.png')\n",
    "\n",
    "print(\"Rich visualizations generated for PPT.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220310c9",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "Transforming time variables into cyclic features and encoding categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('cleaned_bike_data.csv')\n",
    "\n",
    "# --- 0. PRE-PROCESSING / FIXING CATEGORICALS ---\n",
    "print(\"--- Fixing Categorical Data ---\")\n",
    "\n",
    "# Convert dteday to datetime first to recover info\n",
    "df['dteday'] = pd.to_datetime(df['dteday'])\n",
    "\n",
    "# Fix 'mnth' from dteday\n",
    "# Sometimes 'mnth' has '?', but dteday is valid.\n",
    "df['mnth'] = df['dteday'].dt.month\n",
    "print(\"Fixed 'mnth' using dteday.\")\n",
    "\n",
    "# Fix 'yr'\n",
    "# Map years to 0 (2011) and 1 (2012)\n",
    "# If dteday year is 2011 -> 0, 2012 -> 1\n",
    "df['yr'] = df['dteday'].dt.year.map({2011: 0, 2012: 1})\n",
    "print(\"Fixed 'yr' using dteday.\")\n",
    "\n",
    "# Fix 'holiday' ('No', 'Yes', '?')\n",
    "# Replace '?' with mode (usually 'No')\n",
    "mode_holiday = df[df['holiday'] != '?']['holiday'].mode()[0]\n",
    "df['holiday'] = df['holiday'].replace('?', mode_holiday)\n",
    "# Map to 0/1\n",
    "df['holiday'] = df['holiday'].map({'No': 0, 'Yes': 1}).astype(int)\n",
    "print(f\"Fixed 'holiday' (imputed '?' with '{mode_holiday}').\")\n",
    "\n",
    "# Fix 'workingday' ('No work', 'Working Day', '?')\n",
    "# Replace '?' with mode\n",
    "mode_working = df[df['workingday'] != '?']['workingday'].mode()[0]\n",
    "df['workingday'] = df['workingday'].replace('?', mode_working)\n",
    "# Map to 0/1\n",
    "df['workingday'] = df['workingday'].map({'No work': 0, 'Working Day': 1}).astype(int)\n",
    "print(f\"Fixed 'workingday' (imputed '?' with '{mode_working}').\")\n",
    "\n",
    "# Fix 'weekday' just in case (already numeric but good to ensure consistency)\n",
    "# 0: Sunday, 1: Monday... 6: Saturday (pandas .dow is 0=Mon, 6=Sun)\n",
    "# Original dataset: \"Weekday Day of the week\" (0-6). Let's assume standard starts 0.\n",
    "# We can just keep the original column if it was int64, which it was.\n",
    "\n",
    "# --- 1. Feature Engineering ---\n",
    "\n",
    "# A. Categorical Encoding (Season, Weathersit)\n",
    "# Season: springer, summer, fall, winter (from unique values or implied)\n",
    "# Weathersit: 1, 2, 3, 4 (stored as strings or objects in original? check)\n",
    "# Let's ensure 'weathersit' is clean. \n",
    "# It might have '?' too.\n",
    "if df['weathersit'].dtype == object:\n",
    "    # Check for '?'\n",
    "    mode_weather = df[df['weathersit'] != '?']['weathersit'].mode()[0]\n",
    "    df['weathersit'] = df['weathersit'].replace('?', mode_weather)\n",
    "    # Check if they are numeric strings '1','2','3','4' or words.\n",
    "    # Assuming from description 1,2,3,4.\n",
    "    # If they are words, pd.get_dummies handles them. If numbers, we treat as categorical.\n",
    "    print(\"Unique weathersit:\", df['weathersit'].unique())\n",
    "\n",
    "# One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['season', 'weathersit'], prefix=['season', 'weather'], drop_first=True)\n",
    "print(\"Applied One-Hot Encoding.\")\n",
    "\n",
    "# B. Cyclic Encoding\n",
    "def encode_cyclic(df, col, max_val):\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "# Ensure they are numeric\n",
    "df['hr'] = pd.to_numeric(df['hr'], errors='coerce').fillna(0).astype(int)\n",
    "df['mnth'] = pd.to_numeric(df['mnth'], errors='coerce').fillna(1).astype(int)\n",
    "df['weekday'] = pd.to_numeric(df['weekday'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "df = encode_cyclic(df, 'hr', 24)\n",
    "df = encode_cyclic(df, 'mnth', 12)\n",
    "df = encode_cyclic(df, 'weekday', 7)\n",
    "print(\"Applied Cyclic Encoding.\")\n",
    "\n",
    "# C. Scaling\n",
    "scale_cols = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "scaler = MinMaxScaler()\n",
    "df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "print(\"Scaled features.\")\n",
    "\n",
    "# D. Cleanup\n",
    "drop_cols = ['dteday', 'instant']\n",
    "df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# Save\n",
    "df.to_csv('processed_bike_data.csv', index=False)\n",
    "print(\"Saved processed_bike_data.csv\")\n",
    "\n",
    "# Correlation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), annot=False, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix (Processed)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/correlation_matrix_processed.png')\n",
    "print(\"Saved correlation matrix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78822b22",
   "metadata": {},
   "source": [
    "## 4. Model Building\n",
    "Comparing Decision Tree, Random Forest, and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv('processed_bike_data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: processed_bike_data.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "# Define Target and Features\n",
    "# Target is 'cnt'\n",
    "# We must DROP 'casual' and 'registered' because cnt = casual + registered (Data Leakage)\n",
    "target = 'cnt'\n",
    "drop_cols = ['casual', 'registered', 'cnt'] \n",
    "# Note: 'cnt' is in drop_cols just to define X, but we keep it for y.\n",
    "\n",
    "X = df.drop(columns=drop_cols, errors='ignore')\n",
    "y = df[target]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Initialize Models\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Model Training & Evaluation ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R2:   {r2:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    filename = f\"models/{name.replace(' ', '_').lower()}.joblib\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"  Saved model to {filename}\")\n",
    "\n",
    "# Comparison Visualization\n",
    "print(\"\\n--- Generating Comparison Plot ---\")\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df['R2'].plot(kind='barh', color='skyblue')\n",
    "plt.title('Model Comparison - R2 Score')\n",
    "plt.xlabel('R2 Score')\n",
    "plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/model_comparison_r2.png')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df['RMSE'].plot(kind='barh', color='salmon')\n",
    "plt.title('Model Comparison - RMSE (Lower is Better)')\n",
    "plt.xlabel('RMSE')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/model_comparison_rmse.png')\n",
    "\n",
    "print(\"\\nModel building complete. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70235d9f",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning\n",
    "Optimizing the Random Forest model for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e3110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load\n",
    "try:\n",
    "    df = pd.read_csv('processed_bike_data.csv')\n",
    "    print(\"Data loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Run preprocessing first.\")\n",
    "    exit()\n",
    "\n",
    "target = 'cnt'\n",
    "drop_cols = ['casual', 'registered', 'cnt']\n",
    "X = df.drop(columns=drop_cols, errors='ignore')\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Grid for Random Search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "print(\"Starting Randomized Search...\")\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                               n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", rf_random.best_params_)\n",
    "\n",
    "best_rf = rf_random.best_estimator_\n",
    "\n",
    "# Evaluation\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Optimized Model Performance ---\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_rf, 'models/best_random_forest.joblib')\n",
    "print(\"Saved best_random_forest.joblib\")\n",
    "\n",
    "# Feature Importance\n",
    "importances = best_rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Features:\")\n",
    "print(feature_df.head())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plot top 15\n",
    "import seaborn as sns\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_df.head(15))\n",
    "plt.title('Feature Importance (Optimized RF)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/feature_importance.png')\n",
    "print(\"Saved feature_importance.png\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
